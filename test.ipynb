{"nbformat":4,"nbformat_minor":2,"metadata":{"colab":{"name":"test.ipynb","provenance":[],"collapsed_sections":["NtQQwnZcFZAx","D3s1y68D7Fyl","pD4gZfZDC3uj","QU0bHaEzQMQH","2SrRq7VuVJqw"],"toc_visible":true,"mount_file_id":"19_V3qft1FThkuT_NKmUHByCMXoMDIE6r","authorship_tag":"ABX9TyPg1OebVXfCFaLOZDH+CNIC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"d502406824e04693a7497498c41c0f22":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_5af65432bebb47f7ad90af093468c961","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_59aa15f7ae9842dfb7060a3cd9f24b85","IPY_MODEL_2dc49edfa2ac4983a3b44459ad9d7052","IPY_MODEL_957408552ab74567a010e507d8c161e8"]}},"5af65432bebb47f7ad90af093468c961":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"59aa15f7ae9842dfb7060a3cd9f24b85":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_0f54066c0d0d405d9ad06d59e4bc76ae","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f7574dceb212439ab799c94c6f8aa8e1"}},"2dc49edfa2ac4983a3b44459ad9d7052":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_e37c84c4220f4159b12f92173b8dfc2a","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":277138115,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":277138115,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ebc5de3aed37448fb46e7598285f3044"}},"957408552ab74567a010e507d8c161e8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_9613998e40a04bc5aff9264b15a05d89","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 264M/264M [00:25&lt;00:00, 11.4MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ac6be95b9dd040f690b388829f133591"}},"0f54066c0d0d405d9ad06d59e4bc76ae":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"f7574dceb212439ab799c94c6f8aa8e1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e37c84c4220f4159b12f92173b8dfc2a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"ebc5de3aed37448fb46e7598285f3044":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"9613998e40a04bc5aff9264b15a05d89":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"ac6be95b9dd040f690b388829f133591":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","source":["# 装载谷歌云端硬盘"],"metadata":{"id":"NtQQwnZcFZAx"}},{"cell_type":"code","execution_count":1,"source":["# 在谷歌coalb运行\r\n","from google.colab import drive\r\n","drive.mount('/content/gdrive')"],"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xiHDmMsKFel8","executionInfo":{"status":"ok","timestamp":1633767092643,"user_tz":-480,"elapsed":12767,"user":{"displayName":"kangmin xu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16480216821597198117"}},"outputId":"53c5567b-85a6-4881-d45f-e73589ed343b"}},{"cell_type":"markdown","source":["# PyTorchVideo"],"metadata":{"id":"c_uuhcin7GY7"}},{"cell_type":"markdown","source":["## Install PyTorchVideo"],"metadata":{"id":"1TxtXS-D7O5M"}},{"cell_type":"code","execution_count":2,"source":["!pip install pytorchvideo"],"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pytorchvideo\n","  Downloading pytorchvideo-0.1.3.tar.gz (128 kB)\n","\u001b[K     |████████████████████████████████| 128 kB 5.4 MB/s \n","\u001b[?25hCollecting fvcore\n","  Downloading fvcore-0.1.5.post20210924.tar.gz (49 kB)\n","\u001b[K     |████████████████████████████████| 49 kB 5.7 MB/s \n","\u001b[?25hCollecting av\n","  Downloading av-8.0.3-cp37-cp37m-manylinux2010_x86_64.whl (37.2 MB)\n","\u001b[K     |████████████████████████████████| 37.2 MB 32 kB/s \n","\u001b[?25hCollecting parameterized\n","  Downloading parameterized-0.8.1-py2.py3-none-any.whl (26 kB)\n","Collecting iopath\n","  Downloading iopath-0.1.9-py3-none-any.whl (27 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fvcore->pytorchvideo) (1.19.5)\n","Collecting yacs>=0.1.6\n","  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n","\u001b[K     |████████████████████████████████| 636 kB 37.9 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from fvcore->pytorchvideo) (4.62.3)\n","Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.7/dist-packages (from fvcore->pytorchvideo) (1.1.0)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from fvcore->pytorchvideo) (7.1.2)\n","Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from fvcore->pytorchvideo) (0.8.9)\n","Collecting portalocker\n","  Downloading portalocker-2.3.2-py2.py3-none-any.whl (15 kB)\n","Building wheels for collected packages: pytorchvideo, fvcore\n","  Building wheel for pytorchvideo (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pytorchvideo: filename=pytorchvideo-0.1.3-py3-none-any.whl size=183829 sha256=6fe14864e414abddfd407d442162c19f2b4525b995353205e67600faf633d327\n","  Stored in directory: /root/.cache/pip/wheels/d4/a7/4c/bada8b1065ae9befac2da6d7f6648cd6718681eb7901ca226d\n","  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for fvcore: filename=fvcore-0.1.5.post20210924-py3-none-any.whl size=60829 sha256=553755989ade84e8f47e1eae6627aeb1978e1550d3b19633d114ea6a6159c4a4\n","  Stored in directory: /root/.cache/pip/wheels/70/c6/de/aa41c65141bdbc9a8aa4b303ce26482aa2f1720ff41b7f17c3\n","Successfully built pytorchvideo fvcore\n","Installing collected packages: pyyaml, portalocker, yacs, iopath, parameterized, fvcore, av, pytorchvideo\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed av-8.0.3 fvcore-0.1.5.post20210924 iopath-0.1.9 parameterized-0.8.1 portalocker-2.3.2 pytorchvideo-0.1.3 pyyaml-5.4.1 yacs-0.1.8\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qqVrH-6S2EKw","executionInfo":{"status":"ok","timestamp":1633767115295,"user_tz":-480,"elapsed":13154,"user":{"displayName":"kangmin xu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16480216821597198117"}},"outputId":"29090627-8b6f-4495-aeac-0497bb9a64f5"}},{"cell_type":"markdown","source":["## Imports"],"metadata":{"id":"D3s1y68D7Fyl"}},{"cell_type":"code","execution_count":null,"source":["import torch\r\n","import json\r\n","from torchvision.transforms import Compose, Lambda\r\n","from torchvision.transforms._transforms_video import (\r\n","    CenterCropVideo,\r\n","    NormalizeVideo,\r\n",")\r\n","from pytorchvideo.data.encoded_video import EncodedVideo\r\n","from pytorchvideo.transforms import (\r\n","    ApplyTransformToKey,\r\n","    ShortSideScale,\r\n","    UniformTemporalSubsample,\r\n","    UniformCropVideo\r\n",")\r\n","from typing import Dict\r\n","\r\n","import torchvision"],"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torchvision/transforms/_functional_video.py:6: UserWarning: The _functional_video module is deprecated. Please use the functional module instead.\n","  \"The _functional_video module is deprecated. Please use the functional module instead.\"\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/_transforms_video.py:26: UserWarning: The _transforms_video module is deprecated. Please use the transforms module instead.\n","  \"The _transforms_video module is deprecated. Please use the transforms module instead.\"\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xHoTzODl7AOs","executionInfo":{"status":"ok","timestamp":1633765456913,"user_tz":-480,"elapsed":26101,"user":{"displayName":"kangmin xu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16480216821597198117"}},"outputId":"591d0eba-7e31-42f6-b734-76dc59ce05a8"}},{"cell_type":"markdown","source":["## Load model"],"metadata":{"id":"uEetnRdb7dF2"}},{"cell_type":"code","execution_count":null,"source":["# Device on which to run the model\r\n","# Set to cuda to load on GPU\r\n","# Set to cpu to load on CPU\r\n","device = \"cuda\"\r\n","\r\n","# Pick a pretrained model and load the pretrained weights\r\n","model_name = \"slowfast_r50\"\r\n","# This is a bug introduced in pytorch 1.9\r\n","# 这个bug似乎只会出现在colab上\r\n","# https://stackoverflow.com/questions/68901236/urllib-error-httperror-http-error-403-rate-limit-exceeded-when-loading-resnet1\r\n","torch.hub._validate_not_a_forked_repo=lambda a,b,c: True\r\n","# 代码默认主分支是master，github最近的默认主分支都从master迁移到了main\r\n","model = torch.hub.load(\"facebookresearch/pytorchvideo:main\", model=model_name, pretrained=True)\r\n","\r\n","# Set to eval mode and move to desired device\r\n","model = model.to(device)\r\n","model = model.eval()"],"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading: \"https://github.com/facebookresearch/pytorchvideo/archive/main.zip\" to /root/.cache/torch/hub/main.zip\n","Downloading: \"https://dl.fbaipublicfiles.com/pytorchvideo/model_zoo/kinetics/SLOWFAST_8x8_R50.pyth\" to /root/.cache/torch/hub/checkpoints/SLOWFAST_8x8_R50.pyth\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d502406824e04693a7497498c41c0f22","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0.00/264M [00:00<?, ?B/s]"]},"metadata":{}}],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":102,"referenced_widgets":["d502406824e04693a7497498c41c0f22","5af65432bebb47f7ad90af093468c961","59aa15f7ae9842dfb7060a3cd9f24b85","2dc49edfa2ac4983a3b44459ad9d7052","957408552ab74567a010e507d8c161e8","0f54066c0d0d405d9ad06d59e4bc76ae","f7574dceb212439ab799c94c6f8aa8e1","e37c84c4220f4159b12f92173b8dfc2a","ebc5de3aed37448fb46e7598285f3044","9613998e40a04bc5aff9264b15a05d89","ac6be95b9dd040f690b388829f133591"]},"id":"tAnS933D7cJ7","executionInfo":{"status":"ok","timestamp":1633765504276,"user_tz":-480,"elapsed":44284,"user":{"displayName":"kangmin xu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16480216821597198117"}},"outputId":"40d97de5-775d-42fa-af0b-2f06f0633f50"}},{"cell_type":"markdown","source":["## Setup labels"],"metadata":{"id":"7ybzFJ52A6gN"}},{"cell_type":"code","execution_count":null,"source":["!wget https://dl.fbaipublicfiles.com/pyslowfast/dataset/class_names/kinetics_classnames.json"],"outputs":[{"output_type":"stream","name":"stdout","text":["--2021-10-09 07:45:41--  https://dl.fbaipublicfiles.com/pyslowfast/dataset/class_names/kinetics_classnames.json\n","Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.75.142, 104.22.74.142, 172.67.9.4, ...\n","Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.75.142|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 10326 (10K) [text/plain]\n","Saving to: ‘kinetics_classnames.json’\n","\n","kinetics_classnames 100%[===================>]  10.08K  --.-KB/s    in 0s      \n","\n","2021-10-09 07:45:42 (80.4 MB/s) - ‘kinetics_classnames.json’ saved [10326/10326]\n","\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5qBAATZe7oeG","executionInfo":{"status":"ok","timestamp":1633765544755,"user_tz":-480,"elapsed":1559,"user":{"displayName":"kangmin xu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16480216821597198117"}},"outputId":"492ded65-d495-4711-e39c-81514d3b70a8"}},{"cell_type":"code","execution_count":null,"source":["with open(\"kinetics_classnames.json\", \"r\") as f:\r\n","    kinetics_classnames = json.load(f)\r\n","\r\n","# Create an id to label name mapping\r\n","kinetics_id_to_classname = {}\r\n","for k, v in kinetics_classnames.items():\r\n","    kinetics_id_to_classname[v] = str(k).replace('\"', \"\")"],"outputs":[],"metadata":{"id":"GS7HNvMYA8q6"}},{"cell_type":"markdown","source":["## Input transform"],"metadata":{"id":"EM_asC84BD7x"}},{"cell_type":"code","execution_count":null,"source":["####################\r\n","# SlowFast transform\r\n","####################\r\n","\r\n","side_size = 256\r\n","mean = [0.45, 0.45, 0.45]\r\n","std = [0.225, 0.225, 0.225]\r\n","crop_size = 256\r\n","num_frames = 32\r\n","sampling_rate = 2\r\n","frames_per_second = 30\r\n","alpha = 4\r\n","\r\n","class PackPathway(torch.nn.Module):\r\n","    \"\"\"\r\n","    Transform for converting video frames as a list of tensors.\r\n","    \"\"\"\r\n","    def __init__(self):\r\n","        super().__init__()\r\n","\r\n","    def forward(self, frames: torch.Tensor):\r\n","        fast_pathway = frames\r\n","        # Perform temporal sampling from the fast pathway.\r\n","        slow_pathway = torch.index_select(\r\n","            frames,\r\n","            1,\r\n","            torch.linspace(\r\n","                0, frames.shape[1] - 1, frames.shape[1] // alpha\r\n","            ).long(),\r\n","        )\r\n","        frame_list = [slow_pathway, fast_pathway]\r\n","        return frame_list\r\n","\r\n","transform =  ApplyTransformToKey(\r\n","    key=\"video\",\r\n","    transform=Compose(\r\n","        [\r\n","            UniformTemporalSubsample(num_frames),\r\n","            Lambda(lambda x: x/255.0),\r\n","            NormalizeVideo(mean, std),\r\n","            ShortSideScale(\r\n","                size=side_size\r\n","            ),\r\n","            CenterCropVideo(crop_size),\r\n","            PackPathway()\r\n","        ]\r\n","    ),\r\n",")\r\n","\r\n","# The duration of the input clip is also specific to the model.\r\n","clip_duration = (num_frames * sampling_rate)/frames_per_second"],"outputs":[],"metadata":{"id":"z_Z24uSTA_S1"}},{"cell_type":"markdown","source":["## Load an example video"],"metadata":{"id":"xmmkfM5TBI42"}},{"cell_type":"code","execution_count":null,"source":["# Download the example video file\r\n","!wget https://dl.fbaipublicfiles.com/pytorchvideo/projects/archery.mp4"],"outputs":[{"output_type":"stream","name":"stdout","text":["--2021-10-09 07:46:45--  https://dl.fbaipublicfiles.com/pytorchvideo/projects/archery.mp4\n","Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.75.142, 172.67.9.4, 104.22.74.142, ...\n","Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.75.142|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 549197 (536K) [video/mp4]\n","Saving to: ‘archery.mp4’\n","\n","archery.mp4         100%[===================>] 536.33K   722KB/s    in 0.7s    \n","\n","2021-10-09 07:46:47 (722 KB/s) - ‘archery.mp4’ saved [549197/549197]\n","\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qoCRNlpQBGQf","executionInfo":{"status":"ok","timestamp":1633765610440,"user_tz":-480,"elapsed":2430,"user":{"displayName":"kangmin xu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16480216821597198117"}},"outputId":"75a6920f-3e28-4073-9620-c12d1605e925"}},{"cell_type":"code","execution_count":null,"source":["# Load the example video\r\n","video_path = \"archery.mp4\"\r\n","\r\n","# Select the duration of the clip to load by specifying the start and end duration\r\n","# The start_sec should correspond to where the action occurs in the video\r\n","start_sec = 0\r\n","end_sec = start_sec + clip_duration\r\n","\r\n","# Initialize an EncodedVideo helper class\r\n","video = EncodedVideo.from_path(video_path)\r\n","\r\n","# Load the desired clip\r\n","video_data = video.get_clip(start_sec=start_sec, end_sec=end_sec)\r\n","\r\n","# Apply a transform to normalize the video input\r\n","video_data = transform(video_data)\r\n","\r\n","# Move the inputs to the desired device\r\n","inputs = video_data[\"video\"]\r\n","inputs = [i.to(device)[None, ...] for i in inputs]"],"outputs":[],"metadata":{"id":"QVC7tHyEBMfW"}},{"cell_type":"markdown","source":["## Get model prediction"],"metadata":{"id":"ACNUd8s6BRNe"}},{"cell_type":"code","execution_count":null,"source":["# Pass the input clip through the model\r\n","preds = model(inputs)"],"outputs":[],"metadata":{"id":"swIJvfpnBOoI"}},{"cell_type":"code","execution_count":null,"source":["# Get the predicted classes\r\n","post_act = torch.nn.Softmax(dim=1)\r\n","preds = post_act(preds)\r\n","pred_classes = preds.topk(k=5).indices\r\n","\r\n","# Map the predicted classes to the label names\r\n","pred_class_names = [kinetics_id_to_classname[int(i)] for i in pred_classes[0]]\r\n","print(\"Predicted labels: %s\" % \", \".join(pred_class_names))"],"outputs":[{"output_type":"stream","name":"stdout","text":["Predicted labels: archery, throwing axe, playing paintball, disc golfing, riding or walking with horse\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZHynih4PBTvJ","executionInfo":{"status":"ok","timestamp":1633765648656,"user_tz":-480,"elapsed":456,"user":{"displayName":"kangmin xu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16480216821597198117"}},"outputId":"db8706b1-e350-486b-a507-51554a24b966"}},{"cell_type":"code","execution_count":null,"source":[],"outputs":[],"metadata":{"id":"LjK-kN8xBWTI"}},{"cell_type":"markdown","source":["# MMAction2"],"metadata":{"id":"kmqpva9iCxyJ"}},{"cell_type":"markdown","source":["## Install"],"metadata":{"id":"pD4gZfZDC3uj"}},{"cell_type":"code","execution_count":8,"source":["!pip install mmcv"],"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting mmcv\n","  Downloading mmcv-1.3.14.tar.gz (324 kB)\n","\u001b[?25l\r\u001b[K     |█                               | 10 kB 25.2 MB/s eta 0:00:01\r\u001b[K     |██                              | 20 kB 19.2 MB/s eta 0:00:01\r\u001b[K     |███                             | 30 kB 10.3 MB/s eta 0:00:01\r\u001b[K     |████                            | 40 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |█████                           | 51 kB 4.8 MB/s eta 0:00:01\r\u001b[K     |██████                          | 61 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████                         | 71 kB 5.9 MB/s eta 0:00:01\r\u001b[K     |████████                        | 81 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 92 kB 6.2 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 102 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 112 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 122 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 133 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 143 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 153 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 163 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 174 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 184 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 194 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 204 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 215 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 225 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 235 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 245 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 256 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 266 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 276 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 286 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 296 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 307 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 317 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 324 kB 5.4 MB/s \n","\u001b[?25hCollecting addict\n","  Downloading addict-2.4.0-py3-none-any.whl (3.8 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from mmcv) (1.19.5)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from mmcv) (21.0)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from mmcv) (7.1.2)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from mmcv) (5.4.1)\n","Collecting yapf\n","  Downloading yapf-0.31.0-py2.py3-none-any.whl (185 kB)\n","\u001b[K     |████████████████████████████████| 185 kB 38.7 MB/s \n","\u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->mmcv) (2.4.7)\n","Building wheels for collected packages: mmcv\n","  Building wheel for mmcv (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for mmcv: filename=mmcv-1.3.14-py2.py3-none-any.whl size=471262 sha256=9607727e06e1acc0412ba21acccc5413592ccb3e85caea56b63307aac4b628a1\n","  Stored in directory: /root/.cache/pip/wheels/ed/31/c0/8a9c014d146ea76332e5463eba0af8df69d18d49f22878c1ae\n","Successfully built mmcv\n","Installing collected packages: yapf, addict, mmcv\n","Successfully installed addict-2.4.0 mmcv-1.3.14 yapf-0.31.0\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fu8oHLf5C1vy","executionInfo":{"status":"ok","timestamp":1633767434017,"user_tz":-480,"elapsed":11536,"user":{"displayName":"kangmin xu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16480216821597198117"}},"outputId":"6484c16e-10be-4655-a140-8a74fdb7b388"}},{"cell_type":"code","execution_count":10,"source":["!pip install git+https://github.com/open-mmlab/mim.git\r\n","!mim install mmaction2"],"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting git+https://github.com/open-mmlab/mim.git\n","  Cloning https://github.com/open-mmlab/mim.git to /tmp/pip-req-build-ckqg0_12\n","  Running command git clone -q https://github.com/open-mmlab/mim.git /tmp/pip-req-build-ckqg0_12\n","Requirement already satisfied: Click==7.1.2 in /usr/local/lib/python3.7/dist-packages (from openmim==0.1.5) (7.1.2)\n","Collecting colorama\n","  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from openmim==0.1.5) (2.23.0)\n","Collecting model-index\n","  Downloading model_index-0.1.11-py3-none-any.whl (34 kB)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from openmim==0.1.5) (1.1.5)\n","Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from openmim==0.1.5) (0.8.9)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from model-index->openmim==0.1.5) (5.4.1)\n","Requirement already satisfied: markdown in /usr/local/lib/python3.7/dist-packages (from model-index->openmim==0.1.5) (3.3.4)\n","Collecting ordered-set\n","  Downloading ordered-set-4.0.2.tar.gz (10 kB)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown->model-index->openmim==0.1.5) (4.8.1)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown->model-index->openmim==0.1.5) (3.7.4.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown->model-index->openmim==0.1.5) (3.6.0)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->openmim==0.1.5) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->openmim==0.1.5) (2.8.2)\n","Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas->openmim==0.1.5) (1.19.5)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->openmim==0.1.5) (1.15.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->openmim==0.1.5) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->openmim==0.1.5) (2021.5.30)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->openmim==0.1.5) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->openmim==0.1.5) (3.0.4)\n","Building wheels for collected packages: openmim, ordered-set\n","  Building wheel for openmim (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for openmim: filename=openmim-0.1.5-py2.py3-none-any.whl size=46666 sha256=5b08d538ac5056306c4b932824c1d422bc2a65dd63bed24b0b812164079e3ea5\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-9lg7rpy6/wheels/c3/19/91/68ae39ecb699cd4626f6984662f71231a46bfa60cf5bb94631\n","  Building wheel for ordered-set (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for ordered-set: filename=ordered_set-4.0.2-py2.py3-none-any.whl size=8219 sha256=c5d52af55a1b717bd95576710fb1353c0bdf699e9ccdf5234ac26441dcf50705\n","  Stored in directory: /root/.cache/pip/wheels/73/2b/f6/26e9f84153c25050fe7c09e88f8e32a6be3c7034a38c418319\n","Successfully built openmim ordered-set\n","Installing collected packages: ordered-set, model-index, colorama, openmim\n","Successfully installed colorama-0.4.4 model-index-0.1.11 openmim-0.1.5 ordered-set-4.0.2\n","installing mmaction2 from https://github.com/open-mmlab/mmaction2.git.\n","Cloning into '/tmp/tmpt0dog3jk/mmaction2'...\n","remote: Enumerating objects: 13952, done.\u001b[K\n","remote: Counting objects: 100% (207/207), done.\u001b[K\n","remote: Compressing objects: 100% (162/162), done.\u001b[K\n","remote: Total 13952 (delta 57), reused 160 (delta 45), pack-reused 13745\u001b[K\n","Receiving objects: 100% (13952/13952), 44.02 MiB | 30.39 MiB/s, done.\n","Resolving deltas: 100% (10113/10113), done.\n","Note: checking out 'c2725e7a41515ef36d5dd170e337153dac863101'.\n","\n","You are in 'detached HEAD' state. You can look around, make experimental\n","changes and commit them, and you can discard any commits you make in this\n","state without impacting any branches by performing another checkout.\n","\n","If you want to create a new branch to retain commits you create, you may\n","do so (now or later) by using -b with the checkout command again. Example:\n","\n","  git checkout -b <new-branch-name>\n","\n","\u001b[32minstalling dependency: mmcv-full\u001b[0m\n","installing mmcv-full from wheel.\n","Looking in links: https://download.openmmlab.com/mmcv/dist/cu111/torch1.9.0/index.html\n","Collecting mmcv-full==1.3.14\n","  Downloading https://download.openmmlab.com/mmcv/dist/cu111/torch1.9.0/mmcv_full-1.3.14-cp37-cp37m-manylinux1_x86_64.whl (36.7 MB)\n","\u001b[K     |████████████████████████████████| 36.7 MB 32 kB/s \n","\u001b[?25hRequirement already satisfied: yapf in /usr/local/lib/python3.7/dist-packages (from mmcv-full==1.3.14) (0.31.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from mmcv-full==1.3.14) (21.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from mmcv-full==1.3.14) (1.19.5)\n","Requirement already satisfied: opencv-python>=3 in /usr/local/lib/python3.7/dist-packages (from mmcv-full==1.3.14) (4.1.2.30)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from mmcv-full==1.3.14) (7.1.2)\n","Requirement already satisfied: addict in /usr/local/lib/python3.7/dist-packages (from mmcv-full==1.3.14) (2.4.0)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from mmcv-full==1.3.14) (5.4.1)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->mmcv-full==1.3.14) (2.4.7)\n","Installing collected packages: mmcv-full\n","Successfully installed mmcv-full-1.3.14\n","\u001b[32mSuccessfully installed mmcv-full.\u001b[0m\n","\u001b[32mSuccessfully installed dependencies.\u001b[0m\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from -r /tmp/tmpt0dog3jk/mmaction2/requirements/build.txt (line 2)) (1.19.5)\n","Requirement already satisfied: torch>=1.3 in /usr/local/lib/python3.7/dist-packages (from -r /tmp/tmpt0dog3jk/mmaction2/requirements/build.txt (line 3)) (1.9.0+cu111)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.3->-r /tmp/tmpt0dog3jk/mmaction2/requirements/build.txt (line 3)) (3.7.4.3)\n","Processing /tmp/tmpt0dog3jk/mmaction2\n","Collecting decord\n","  Downloading decord-0.6.0-py3-none-manylinux2010_x86_64.whl (13.6 MB)\n","\u001b[K     |████████████████████████████████| 13.6 MB 55 kB/s \n","\u001b[?25hCollecting einops\n","  Downloading einops-0.3.2-py3-none-any.whl (25 kB)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from mmaction2==0.19.0) (3.2.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from mmaction2==0.19.0) (1.19.5)\n","Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.7/dist-packages (from mmaction2==0.19.0) (4.1.2.30)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from mmaction2==0.19.0) (7.1.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from mmaction2==0.19.0) (1.4.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mmaction2==0.19.0) (0.10.0)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mmaction2==0.19.0) (2.8.2)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mmaction2==0.19.0) (2.4.7)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mmaction2==0.19.0) (1.3.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib->mmaction2==0.19.0) (1.15.0)\n","Building wheels for collected packages: mmaction2\n","  Building wheel for mmaction2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for mmaction2: filename=mmaction2-0.19.0-py2.py3-none-any.whl size=720800 sha256=60f4be2c7885a083c0875b9439075a501e4929e4d9cdb9b09a6ea448f22cc681\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-4o2xyzrb/wheels/8c/63/91/4a4d60838761a91afc3aa352b422df373ca9ad196e5b50d66a\n","Successfully built mmaction2\n","Installing collected packages: einops, decord, mmaction2\n","Successfully installed decord-0.6.0 einops-0.3.2 mmaction2-0.19.0\n","\u001b[32mSuccessfully installed mmaction2.\u001b[0m\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"85Imq9tCDd0k","executionInfo":{"status":"ok","timestamp":1633767516121,"user_tz":-480,"elapsed":39344,"user":{"displayName":"kangmin xu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16480216821597198117"}},"outputId":"db546112-cbc5-4e6c-8a8d-06eace6bc45d"}},{"cell_type":"code","execution_count":6,"source":["# 将mmaction2下载到云端硬盘中\r\n","# 地址修改为运行时的地址\r\n","!git clone https://github.com/open-mmlab/mmaction2.git /content/gdrive/MyDrive/ColabNotebooks/benchmarking_pytorchvideo_and_mmaction2/mmaction2"],"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into '/content/gdrive/MyDrive/ColabNotebooks/benchmarking_pytorchvideo_and_mmaction2/mmaction2'...\n","remote: Enumerating objects: 13952, done.\u001b[K\n","remote: Counting objects: 100% (207/207), done.\u001b[K\n","remote: Compressing objects: 100% (162/162), done.\u001b[K\n","remote: Total 13952 (delta 57), reused 160 (delta 45), pack-reused 13745\u001b[K\n","Receiving objects: 100% (13952/13952), 44.02 MiB | 10.78 MiB/s, done.\n","Resolving deltas: 100% (10111/10111), done.\n","Checking out files: 100% (909/909), done.\n"]}],"metadata":{"id":"5eiPzgfZFyry","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1633767196819,"user_tz":-480,"elapsed":19139,"user":{"displayName":"kangmin xu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16480216821597198117"}},"outputId":"8632c9ab-7195-4856-893e-ed6df3279c24"}},{"cell_type":"code","execution_count":13,"source":["# 验证安装\r\n","import torch\r\n","from mmaction.apis import init_recognizer, inference_recognizer\r\n","\r\n","# 云端硬盘中的路径\r\n","# config_file为mmaction2的下载地址\r\n","config_file = '/content/gdrive/MyDrive/ColabNotebooks/benchmarking_pytorchvideo_and_mmaction2/mmaction2/configs/recognition/tsn/tsn_r50_video_inference_1x1x3_100e_kinetics400_rgb.py'\r\n","device = 'cuda:0' # 或 'cpu'\r\n","device = torch.device(device)\r\n","\r\n","model = init_recognizer(config_file, device=device)\r\n","# 进行演示视频的推理\r\n","inference_recognizer(model, '/content/gdrive/MyDrive/ColabNotebooks/benchmarking_pytorchvideo_and_mmaction2/mmaction2/demo/demo.mp4')"],"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n","  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"]},{"output_type":"execute_result","data":{"text/plain":["[(128, 67.84383),\n"," (398, 57.854355),\n"," (6, 57.40088),\n"," (276, 49.40184),\n"," (363, 49.26752)]"]},"metadata":{},"execution_count":13}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-aaS_BvGEi-t","executionInfo":{"status":"ok","timestamp":1633768285243,"user_tz":-480,"elapsed":4596,"user":{"displayName":"kangmin xu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16480216821597198117"}},"outputId":"bc4aea49-ed83-492e-a635-75e12b697e94"}},{"cell_type":"markdown","source":["## Preparation"],"metadata":{"id":"QU0bHaEzQMQH"}},{"cell_type":"code","execution_count":34,"source":["# 从模型库中下载并放到checkpoints文件下\r\n","# 后面为运行时具体的地址\r\n","!wget https://download.openmmlab.com/mmaction/recognition/tsn/tsn_r50_1x1x3_100e_kinetics400_rgb/tsn_r50_1x1x3_100e_kinetics400_rgb_20200614-e508be42.pth -P /content/gdrive/MyDrive/ColabNotebooks/benchmarking_pytorchvideo_and_mmaction2/mmaction2/checkpoints/"],"outputs":[{"output_type":"stream","name":"stdout","text":["--2021-10-09 09:12:47--  https://download.openmmlab.com/mmaction/recognition/tsn/tsn_r50_1x1x3_100e_kinetics400_rgb/tsn_r50_1x1x3_100e_kinetics400_rgb_20200614-e508be42.pth\n","Resolving download.openmmlab.com (download.openmmlab.com)... 47.252.96.35\n","Connecting to download.openmmlab.com (download.openmmlab.com)|47.252.96.35|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 97579339 (93M) [application/octet-stream]\n","Saving to: ‘/content/gdrive/MyDrive/ColabNotebooks/benchmarking_pytorchvideo_and_mmaction2/mmaction2/checkpoints/tsn_r50_1x1x3_100e_kinetics400_rgb_20200614-e508be42.pth’\n","\n","tsn_r50_1x1x3_100e_ 100%[===================>]  93.06M  8.22MB/s    in 12s     \n","\n","2021-10-09 09:13:01 (7.63 MB/s) - ‘/content/gdrive/MyDrive/ColabNotebooks/benchmarking_pytorchvideo_and_mmaction2/mmaction2/checkpoints/tsn_r50_1x1x3_100e_kinetics400_rgb_20200614-e508be42.pth’ saved [97579339/97579339]\n","\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8e1KNa5PQkXq","executionInfo":{"status":"ok","timestamp":1633770783771,"user_tz":-480,"elapsed":13946,"user":{"displayName":"kangmin xu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16480216821597198117"}},"outputId":"6eae8ce9-a27b-401b-e436-3e0067e170de"}},{"cell_type":"markdown","source":["## Imports"],"metadata":{"id":"Uxza18kUOO8L"}},{"cell_type":"code","execution_count":37,"source":["import os\r\n","# 运行时目录改为mmaction2所在的目录\r\n","os.environ['MMACTION2'] = '/content/gdrive/MyDrive/ColabNotebooks/benchmarking_pytorchvideo_and_mmaction2/mmaction2'\r\n","os.chdir('/content/gdrive/MyDrive/ColabNotebooks/benchmarking_pytorchvideo_and_mmaction2/mmaction2')"],"outputs":[],"metadata":{"id":"usv5p1kTNizp","executionInfo":{"status":"ok","timestamp":1633770935700,"user_tz":-480,"elapsed":453,"user":{"displayName":"kangmin xu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16480216821597198117"}}}},{"cell_type":"code","execution_count":38,"source":["import torch\r\n","\r\n","from mmaction.apis import init_recognizer, inference_recognizer"],"outputs":[],"metadata":{"id":"43fV5N1bOiC4","executionInfo":{"status":"ok","timestamp":1633770937144,"user_tz":-480,"elapsed":6,"user":{"displayName":"kangmin xu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16480216821597198117"}}}},{"cell_type":"markdown","source":["## Load model"],"metadata":{"id":"9lnkOCsTTOg4"}},{"cell_type":"code","execution_count":39,"source":["config_file = 'configs/recognition/tsn/tsn_r50_video_inference_1x1x3_100e_kinetics400_rgb.py'\r\n","# 从模型库中下载检测点，并把它放到 `checkpoints/` 文件夹下\r\n","checkpoint_file = 'checkpoints/tsn_r50_1x1x3_100e_kinetics400_rgb_20200614-e508be42.pth'\r\n","\r\n","# 指定设备\r\n","device = 'cuda:0' # or 'cpu'\r\n","device = torch.device(device)\r\n","\r\n"," # 根据配置文件和检查点来建立模型\r\n","model = init_recognizer(config_file, checkpoint_file, device=device)"],"outputs":[{"output_type":"stream","name":"stdout","text":["Use load_from_local loader\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nrGhKFkWQDPY","executionInfo":{"status":"ok","timestamp":1633770940456,"user_tz":-480,"elapsed":1243,"user":{"displayName":"kangmin xu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16480216821597198117"}},"outputId":"1c9e4807-7e00-4190-8ab9-c3978b342927"}},{"cell_type":"markdown","source":["## Get model prediction"],"metadata":{"id":"2SrRq7VuVJqw"}},{"cell_type":"code","execution_count":44,"source":["# 测试单个视频并显示其结果\r\n","video = 'demo/demo.mp4'\r\n","# labels = 'tools/data/kinetics/label_map_k400.txt'\r\n","results = inference_recognizer(model, video)"],"outputs":[],"metadata":{"id":"5V84CWYdTUu0","executionInfo":{"status":"ok","timestamp":1633771341782,"user_tz":-480,"elapsed":3763,"user":{"displayName":"kangmin xu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16480216821597198117"}}}},{"cell_type":"code","execution_count":45,"source":["# 显示结果\r\n","labels = open('tools/data/kinetics/label_map_k400.txt').readlines()\r\n","labels = [x.strip() for x in labels]\r\n","results = [(labels[k[0]], k[1]) for k in results]\r\n","\r\n","print(f'The top-5 labels with corresponding scores are:')\r\n","for result in results:\r\n","    print(f'{result[0]}: ', result[1])"],"outputs":[{"output_type":"stream","name":"stdout","text":["The top-5 labels with corresponding scores are:\n","arm wrestling:  29.616438\n","rock scissors paper:  10.754841\n","shaking hands:  9.908401\n","clapping:  9.189911\n","massaging feet:  8.305306\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V8mGYbLFVOJf","executionInfo":{"status":"ok","timestamp":1633771344624,"user_tz":-480,"elapsed":305,"user":{"displayName":"kangmin xu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16480216821597198117"}},"outputId":"ce3161d7-a904-469e-aea1-1c5fff036272"}},{"cell_type":"code","execution_count":null,"source":[],"outputs":[],"metadata":{"id":"0Buc7IO6XE_m"}}]}